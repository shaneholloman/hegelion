# Claude Handoff: Hegelion MVB Benchmark

## Context
You're building a **Minimum Viable Benchmark (MVB)** to answer: **Does Hegelion's 3-call dialectic structure produce better reasoning than a single enhanced prompt?**

## What's Been Done

### 1. Scripts Created (all in `benchmarks/scripts/`)
- `select_prompts.py` - Original selection (not ideal)
- `reselect_with_existing.py` - **USE THIS** - Prioritizes prompts with existing Hegelion data
- `generate_responses.py` - Generates raw/enhanced baselines (needs API key - won't work)
- `evaluate.py` - Blind Opus evaluation
- `generate_report.py` - Statistical analysis
- `generate_via_subagents.py` - Prompt templates for subagent generation

### 2. Data Created
- `benchmarks/data/selected_prompts.json` - 50 prompts selected:
  - **41 have existing Hegelion data** (reuse from `hegelion-data/data/hegelion_dialectical_500.jsonl`)
  - **9 need generation** (8 creative + 1 factual)

### 3. Key Decisions Made
- **Hegelion responses**: Use existing data from `hegelion-data/data/hegelion_dialectical_500.jsonl` (laboriously generated by Sonnet)
- **Response lengths**: Real Hegelion = ~1300 chars thesis, ~2000 chars antithesis, ~3600 chars synthesis
- **Baselines**: Generate via `sonnet-test-agent` subagent with proper token limits
- **For missing 9 prompts**: Use MCP server tools (`thesis_prompt`, `antithesis_prompt`, `synthesis_prompt`) + `sonnet-test-agent`
- **Evaluation**: Use Opus as blind judge (I am Opus - can do this directly or via subagent)

## What Needs to Be Done

### Step 1: Extract Existing Hegelion Data
Write script to copy the 41 matching Hegelion responses from `hegelion-data/data/hegelion_dialectical_500.jsonl` to `benchmarks/data/responses/hegelion/P001.json` etc.

### Step 2: Generate Missing 9 Hegelion Responses
For each of the 9 missing prompts, use the MCP server approach:
1. Call `mcp__hegelion__thesis_prompt` to get thesis prompt
2. Spawn `sonnet-test-agent` with that prompt → get thesis (~1300 chars)
3. Call `mcp__hegelion__antithesis_prompt` with query + thesis
4. Spawn `sonnet-test-agent` → get antithesis (~2000 chars)
5. Call `mcp__hegelion__synthesis_prompt` with query + thesis + antithesis
6. Spawn `sonnet-test-agent` → get synthesis (~3600 chars)
7. Save combined response

### Step 3: Generate Baselines (41 + 9 = 50 prompts)
For each prompt, spawn `sonnet-test-agent`:
- **Raw baseline**: Just the question, ~1500 char response
- **Enhanced baseline**: Question + structured thinking system prompt, ~4000 char response

### Step 4: Run Evaluation
Use `evaluate.py` logic but with you (Opus) as the judge, or spawn evaluation agents.

### Step 5: Generate Report
Run `generate_report.py` to create `benchmarks/results/BENCHMARK_REPORT.md`

## Critical Files

| File | Purpose |
|------|---------|
| `benchmarks/data/selected_prompts.json` | 50 prompts with `has_existing_hegelion` flag |
| `hegelion-data/data/hegelion_dialectical_500.jsonl` | 176 existing Hegelion responses |
| `hegelion/mcp/server.py` | MCP tools: `thesis_prompt`, `antithesis_prompt`, `synthesis_prompt` |
| `hegelion/core/prompt_dialectic.py` | The actual prompt generation logic |

## The 9 Missing Prompts (need Hegelion generation)
```
P038: Can AI-generated art be truly creative?
P039: Is street art vandalism or art?
P040: Should art be separate from politics?
P041: Is censoring art ever justified?
P042: Can anything be art?
P043: Should we preserve all cultural heritage?
P044: Is representation in art morally required?
P045: Should artists be held to moral standards?
P050: Is group selection a valid mechanism?
```

## Key Insight Being Tested
**Control theory for LLMs via prompting**: Does staged intervention (redirecting the model at each phase) add value over giving the same instructions in a single longer turn?

- **Enhanced Baseline**: Single prompt asking for thesis → antithesis → synthesis
- **Hegelion**: Three separate calls with forced redirection between phases

## Response Length Targets
From real Hegelion data:
- Thesis: ~1300 characters
- Antithesis: ~2000 characters
- Synthesis: ~3600 characters
- Raw baseline: ~1500 characters
- Enhanced baseline: ~4000 characters

## Important: Use Subagents Correctly
- Use `sonnet-test-agent` for Sonnet responses (unbiased, no benchmark context)
- Use proper token/character limits to match real data
- The MCP server provides **prompts** - you execute them via subagents

## Quick Start Command
```bash
cd /Volumes/VIXinSSD/hegelion
cat benchmarks/data/selected_prompts.json | jq '.prompts[] | select(.has_existing_hegelion == false) | .text'
```

This shows the 9 prompts needing generation.
