Metadata-Version: 2.4
Name: hegelion-server
Version: 0.1.0
Summary: Backend-agnostic MCP server implementing Hegelian dialectical reasoning
Author-email: Hegelion <ops@hegelion.io>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: mcp>=0.9.0
Requires-Dist: openai>=1.0.0
Requires-Dist: anthropic>=0.40.0
Requires-Dist: httpx>=0.25.0
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: scikit-learn>=1.2.0
Requires-Dist: pydantic>=2.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21; extra == "dev"

# Hegelion

```
/==============================================================\
|   HEGELION: A PERMANENT OPPOSITION CO-PROCESSOR FOR YOUR LLM |
\==============================================================/
```

> **A dialectical reasoning coprocessor for LLMs**  
> Thesis → Antithesis → Synthesis, with conflict scores, contradictions, and research proposals as JSON.

Hegelion is an MCP server that adds **structured Hegelian dialectics** to any LLM stack.

Instead of a single model answer, Hegelion runs a **three-phase loop**:

1. **Thesis** – generate a comprehensive answer  
2. **Antithesis** – attack that answer, surfacing contradictions and missing assumptions  
3. **Synthesis** – when conflict is high, construct a higher-level resolution with **novel predictions**

The server exposes this as a single MCP tool, `hegelion_query`, returning a **full dialectical trace** in JSON: thesis, antithesis, optional synthesis, conflict score, timing, and any research proposals extracted from the synthesis.

You can plug Hegelion into Claude Desktop, Cursor, or any MCP-capable client and let your preferred model **choose when to think dialectically**.

---

## Why Hegelion?

LLMs today are often:

- **Confident even when they’re wrong**
- **Under-opposed** – “self-reflection” is usually just another pass of the same model
- **Opaque** – you rarely see where internal disagreement lives

A lot of people have experimented with “debate” and “self-critique”, but most implementations are:

- Ad-hoc – “we asked it to critique itself once”
- Hidden – you never see the internal arguments
- Hard to reuse across stacks

Hegelion turns dialectic into a **first-class, inspectable protocol**:

- **Named phases**: `THESIS → ANTITHESIS → SYNTHESIS`
- **Quantified**: `conflict_score ∈ [0, 1]` combining semantic distance + contradiction count
- **Transparent**: full trace as JSON you can log, analyze, or feed into other tools
- **Backend-agnostic**: works with OpenAI, Anthropic, or local models behind a unified interface

When you want your agent to stop hand-waving and actually **argue with itself**, you call Hegelion.

---

## What Hegelion Does

For each `hegelion_query` call, Hegelion:

### 1. Generates a Thesis

A comprehensive, multi-perspective answer to the original question.

### 2. Generates an Antithesis

A critique that:

- Finds contradictions and logical gaps  
- Surfaces unexamined assumptions  
- Proposes alternative framings  
- Lists explicit contradictions in a machine-readable format

### 3. Computes a Conflict Score

- Uses **sentence-transformer embeddings** to measure semantic distance  
- Adds a **contradiction bonus** based on how many contradictions were found  
- Clamps to `[0, 1]` as `conflict_score`

### 4. Optionally Generates a Synthesis

If `conflict_score >= synthesis_threshold` (default `0.85`), Hegelion:

- Generates a **synthesis** that must *transcend* both thesis and antithesis  
- Must not just “pick a side” or say “both are partly right”  
- Is encouraged to output **research proposals** with falsifiable predictions

### 5. Returns a Structured Trace

```jsonc
{
  "result": "Final answer (synthesis if generated, else thesis)",
  "mode": "synthesis", // or "thesis_only"
  "conflict_score": 0.91,
  "trace": {
    "thesis": "...",
    "antithesis": "...",
    "synthesis": "...",
    "contradictions_found": 4,
    "research_proposals": [
      "RESEARCH_PROPOSAL: ...",
      "..."
    ]
  },
  "metadata": {
    "thesis_time_ms": 340,
    "antithesis_time_ms": 510,
    "synthesis_time_ms": 620,
    "total_time_ms": 1470
  }
}
```

---

## Architecture (v0.1)

This implementation focuses on the **dialectical kernel**:

* **MCP server** written in Python, using the MCP Python SDK
* **LLM backend abstraction**:

  * OpenAI-compatible APIs (e.g. `gpt-4.1`, `gpt-4o`, local OpenAI-compatible gateways)
  * Anthropic (`claude-3.x`) support
  * Other providers can be added behind the same interface
* **Semantic similarity** via `sentence-transformers` (e.g. `all-MiniLM-L6-v2`)
* **Structured prompts** for:

  * THESIS
  * ANTITHESIS (with machine-readable `CONTRADICTION:` blocks)
  * SYNTHESIS (with `RESEARCH_PROPOSAL:` and `TESTABLE_PREDICTION:` markers)

Future versions can layer on:

* Multi-expert routing (**true DEOE**)
* Session-level **“permanent opposition”** across time
* **Constitutional governance / quest system**

---

## Installation

You can install and run Hegelion with `uv` or plain `pip`.

### With `uv` (recommended)

```bash
git clone https://github.com/your-org/hegelion-server.git
cd hegelion-server

uv sync
uv run hegelion-server
```

### With `pip`

```bash
git clone https://github.com/your-org/hegelion-server.git
cd hegelion-server

python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

pip install -e .
hegelion-server
```

---

## Configuration

Hegelion is **backend-agnostic**. You choose provider + model via environment variables.

### Required

Choose a provider (or let it auto-detect):

```bash
export HEGELION_PROVIDER=openai     # or anthropic, ollama, auto
export HEGELION_MODEL=gpt-4.1-mini  # backend-specific
```

Set provider credentials:

* **For OpenAI-compatible APIs:**

  ```bash
  export OPENAI_API_KEY=sk-...
  # Optional:
  # export OPENAI_BASE_URL=https://api.openai.com/v1
  # export OPENAI_ORG_ID=...
  ```

* **For Anthropic:**

  ```bash
  export ANTHROPIC_API_KEY=sk-ant-...
  ```

### Optional

Override synthesis threshold:

```bash
export HEGELION_SYNTHESIS_THRESHOLD=0.85
```

Override max tokens per phase:

```bash
export HEGELION_MAX_TOKENS_PER_PHASE=10000
```

---

## Running the MCP Server

Once dependencies and env vars are set:

```bash
hegelion-server
```

or, with `uv`:

```bash
uv run hegelion-server
```

The server speaks MCP over stdio and can be discovered by any MCP-capable client.

---

## Local CLI Smoke Test

You can exercise the full thesis → antithesis → synthesis loop without an MCP client by running:

```bash
uv run scripts/run_hegelion_query.py "Can AI be genuinely creative?"
```

The script uses the same env-driven backend selection as the server and prints the JSON trace (or writes it to a file via `--output result.json`). This is the fastest way to sanity-check credentials—e.g., pointing `OPENAI_BASE_URL` at your `z.ai` deployment with `HEGELION_MODEL=glm-4.6`.

---

## Batch Logging for Paper Experiments

To reproduce the evaluation workflow described in the paper draft, use the benchmark helper:

```bash
uv run scripts/run_benchmark.py --output logs/hegelion_runs.jsonl
```

It iterates through factual, philosophical, and scientific prompts (or a JSON file you provide via `--queries`) and streams every trace into a JSONL log. The log captures category, timestamps, the original query, and the structured output so you can compute conflict-score distributions, synthesis rates, or research-proposal counts offline.

---

## MCP Integration

### Claude Desktop Example

Add Hegelion to your Desktop config (macOS):

`~/Library/Application Support/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "hegelion": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/hegelion-server",
        "run",
        "hegelion-server"
      ],
      "env": {
        "HEGELION_PROVIDER": "openai",
        "HEGELION_MODEL": "gpt-4.1-mini",
        "OPENAI_API_KEY": "your-openai-key"
      }
    }
  }
}
```

Restart Claude Desktop and it should automatically discover the `hegelion_query` tool.

---

### Claude Code CLI (z.ai GLM‑4.6) Example

If you are running the `claude-code` CLI (which can route to alternate OpenAI-compatible providers like `z.ai`'s `glm-4.6`), add Hegelion to `~/.claude/claude_code_config.json` (or whatever your CLI uses) so that the CLI model can call the MCP server:

```json
{
  "mcpServers": {
    "hegelion": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/hegelion-server",
        "run",
        "hegelion-server"
      ],
      "env": {
        "HEGELION_PROVIDER": "openai",
        "HEGELION_MODEL": "glm-4.6",
        "OPENAI_BASE_URL": "https://api.z.ai/v1",
        "OPENAI_API_KEY": "YOUR_Z_AI_KEY"
      }
    }
  },
  "model": "glm-4.6"
}
```

The CLI will continue answering normally, but whenever the conversation calls for dialectical reasoning it can invoke `hegelion_query`. The same config lives under `examples/claude_code_cli.json`—copy it, update the `--directory` path, and drop in your API key.

---

## Tool Definition (Conceptual)

The server exposes one tool:

* **Name**: `hegelion_query`
* **Use when**: you want deep disagreement + synthesis, or possible research ideas.

**Input schema:**

```json
{
  "type": "object",
  "properties": {
    "query": {
      "type": "string",
      "description": "The question or topic to analyze dialectically"
    },
    "synthesis_threshold": {
      "type": "number",
      "description": "Conflict score threshold for triggering synthesis (0–1, default 0.85)",
      "default": 0.85
    },
    "max_iterations": {
      "type": "number",
      "description": "Maximum dialectical cycles (currently only 1 is implemented)",
      "default": 1
    }
  },
  "required": ["query"]
}
```

---

## Usage Examples

### Example 1 – Philosophical Question

> **Prompt to your LLM (e.g. Claude):**  
> “Use the Hegelion tool to analyze whether AI can be genuinely creative.”

Client calls:

```json
{
  "tool": "hegelion_query",
  "arguments": {
    "query": "Can AI be genuinely creative?"
  }
}
```

Sample response (truncated):

```json
{
  "mode": "synthesis",
  "conflict_score": 0.91,
  "result": "Creativity is better understood as...",
  "trace": {
    "thesis": "AI can be considered creative because it generates novel combinations...",
    "antithesis": "AI lacks subjective experience and intrinsic intention...",
    "synthesis": "Both sides treat creativity as all-or-nothing. A more useful view is...",
    "contradictions_found": 3,
    "research_proposals": [
      "RESEARCH_PROPOSAL: Compare human and AI-generated works under blind evaluation...",
      "TESTABLE_PREDICTION: In domains with low prior exposure, expert judges will..."
    ]
  }
}
```

### Example 2 – Simple Factual Query

> Query: `"What is 2+2?"`

```json
{
  "mode": "thesis_only",
  "conflict_score": 0.12,
  "result": "2+2 = 4.",
  "trace": {
    "thesis": "2+2 = 4 by the standard definition of addition...",
    "antithesis": "There are no meaningful contradictions for basic arithmetic in this context.",
    "synthesis": null,
    "contradictions_found": 0,
    "research_proposals": []
  }
}
```

### Example 3 – Scientific Tension

> Query: `"How do proteins fold so quickly despite such a large configuration space?"`

For rich scientific or philosophical tensions, you should typically see:

* A **high conflict score**
* A synthesis that surfaces a **new framing**
* At least one **research proposal** with a testable prediction

---

## When to Call Hegelion

Hegelion is most useful when:

* The question is **contested, ambiguous, or high-stakes**:

  * ethics, law, policy, strategy, research directions, alignment proposals
* You want:

  * Genuine **tension** between viewpoints
  * Explicit **contradictions** and **edge cases**
  * A structured attempt at **higher-order synthesis**

It’s usually overkill for:

* Simple factual queries
* Straightforward transformations (summarization, formatting)

Use it as a **dialectical mode** your agent can drop into when it senses:

> “This is the kind of question where disagreement and synthesis matter.”

---

## Roadmap

Planned directions:

* **Session-level “permanent opposition”**

  * Track conflict trajectories across queries via `session_id`
* **Multi-expert Thesis/Antithesis**

  * Different models / specialists feeding into each phase
* **Constitutional governance**

  * A `constitution.json` and amendment log
  * Voting-style changes to how synthesis is allowed to behave
* **Quest-style self-discovery**

  * Track per-user “stages” of interaction and meta-questions

The current release focuses on the **dialectical kernel**: a clean, reusable `THESIS → ANTITHESIS → SYNTHESIS` loop exposed as an MCP tool.

---

## Development & Testing

Basic test entry points live under `tests/`:

* Low-conflict factual queries → should remain `thesis_only`
* High-conflict philosophical / scientific queries → should usually trigger `synthesis`
* Synthesis outputs → should contain `RESEARCH_PROPOSAL` markers for rich questions

Run tests, for example:

```bash
uv run pytest
# or
pytest
```

---

## Hegelion v0.1 Diagrams

These diagrams correspond to the v0.1 implementation described above.

### Diagram 1: `hegelion_query` Tool Flow

```mermaid
flowchart TD
    subgraph HegelionQueryFlow [Hegelion v0.1: hegelion_query Tool Flow]
        IN[MCP client calls<br/>hegelion_query(query)]
            --> P1[1. Generate Thesis]

        P1 --> P2[2. Generate Antithesis<br/>(finds contradictions)]
        P2 --> P3[3. Compute Conflict Score<br/>(embeddings + contradiction bonus)]
        P3 --> C{Conflict Score >= 0.85?}

        C -- No --> OUT_T[Final answer = Thesis]
        OUT_T --> FINAL_T[5. Return JSON trace<br/>(mode: thesis_only)]

        C -- Yes --> P4[4. Generate Synthesis<br/>(incl. research proposals)]
        P4 --> OUT_S[Final answer = Synthesis]
        OUT_S --> FINAL_S[5. Return JSON trace<br/>(mode: synthesis)]
    end

    style P1 fill:#e6ffed,stroke:#006d2c
    style P2 fill:#ffebe6,stroke:#c53030
    style P4 fill:#fffbeb,stroke:#b45309
```

### Diagram 2: v0.1 System Architecture

```mermaid
graph TD
    subgraph SystemArchitecture [Hegelion v0.1: System Architecture]
        Client[MCP Client<br/>(e.g., Claude, Cursor)]
        Server[Hegelion MCP Server<br/>(Python)]
        LLM[LLM Backend Abstraction]
        ST[SentenceTransformers<br/>(all-MiniLM-L6-v2)]

        Client --> |hegelion_query| Server
        Server --> |Prompts| LLM
        Server --> |Similarity calc| ST

        LLM --> OpenAI[OpenAI API]
        LLM --> Anthropic[Anthropic API]
        LLM --> Local[Ollama / local API]
    end
```

---

## License

TBD (MIT / Apache-2.0 suggested).

---

## Status

**Experimental.**  
Pull requests, issues, and example traces are welcome.
